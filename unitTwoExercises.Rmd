---
title: "ISLR 2.4 Exercises"
author: "Brent Lockee"
date: "Tuesday, March 22, 2016"
output: html_document
---

1. For each of parts (a) through (d), indicate whether we would generally
expect the performance of a flexible statistical learning method to be
better or worse than an inflexible method. Justify your answer.

(a) The sample size n is extremely large, and the number of predictors
p is small.
    A flexible learning method could be expected to perform better because of the large number
    of samples available.

(b) The number of predictors p is extremely large, and the number
of observations n is small.
    An inflexible method would likely perform better because a flexible model would likely
    overfit the small number of samples.

(c) The relationship between the predictors and response is highly
non-linear.
    A flexible model would likely perform better. An inflexible model would introduce a high
    degree of bias because of the disconnect between the model and the underlying data.

(d) The variance of the error terms, i.e. s2 = Var(), is extremely
high.
    An inflexible model would be preferred because an inflexible model would overfit to
    the large variance of the error.
    
2. Explain whether each scenario is a classification or regression problem,
and indicate whether we are most interested in inference or prediction.
Finally, provide n and p.

(a) We collect a set of data on the top 500 firms in the US. For each
firm we record profit, number of employees, industry and the
CEO salary. We are interested in understanding which factors
affect CEO salary.
    This is a regression problem with n = 500, p = 4 (profit, # employees, industry, CEO salary)

(b) We are considering launching a new product and wish to know
whether it will be a success or a failure. We collect data on 20
similar products that were previously launched. For each product
we have recorded whether it was a success or failure, price
charged for the product, marketing budget, competition price,
and ten other variables.
    Classification. n = 20, p = 14

(c) We are interesting in predicting the % change in the US dollar in
relation to the weekly changes in the world stock markets. Hence
we collect weekly data for all of 2012. For each week we record
the % change in the dollar, the % change in the US market,
the % change in the British market, and the % change in the
German market.
    Regression. n = 52, p = 4

3. We now revisit the bias-variance decomposition.

(a) Provide a sketch of typical (squared) bias, variance, training error,
test error, and Bayes (or irreducible) error curves, on a single
plot, as we go from less flexible statistical learning methods
towards more flexible approaches. The x-axis should represent
the amount of flexibility in the method, and the y-axis should
represent the values for each curve. There should be five curves.
Make sure to label each one.

(b) Explain why each of the five curves has the shape displayed in
part (a).

4. You will now think of some real-life applications for statistical learning.

(a) Describe three real-life applications in which classification might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your
answer.
    Predicting whether or not a customer will renew a subscription. Response - renewal yes/no
    Predictors - customer transaction data, demographic data, customer product use history
    
    Identifying patients with elevated disease risk. Inference.
    Response - presence of disease yes/no
    Predictors - patient health data (HR, smoker?, lab stats), demographic data (age, weight, race)
    
    Detecting fraudulent transactions. Prediction.
    Response - fraudulent/legitimate
    Predictors - transaction history, transaction details (amount, location, time)

(b) Describe three real-life applications in which regression might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your
answer.

    Demand for a product. Prediction.
    Response - # of units sold
    Predictors - Historical demand, time of year, advertising spend, price
    
    Student test scores. Inference if looking for factors associated with high scores.
    Response - Test scores
    Predictors - School size, neighborhood demographics, student demographics, family income
    
    Stock Price. Prediction
    Response - Stock price
    Predictors - Company size, earnings, expenses, stock price history.

(c) Describe three real-life applications in which cluster analysis
might be useful.

    Recommendation engines - cluster users in order to recommend products rated
    highly by similar users.
    
    Cluster baseball hitters to find players with similar offensive profiles
    based on batting average, slugging, OBP, etc.
    
    Cluster prospective customers to understand who might react to marketing
    in a similar fashion and to tune the message to the audience.

5. What are the advantages and disadvantages of a very flexible (versus
a less flexible) approach for regression or classification? 
    A flexible approach enables you to find relationships that wouldn't be identified with a more rigid model. It is well suited to prediction, as it may not be very interpretable for inference. It allows for you to take fuller advantage of large datasets

Under what circumstances might a more flexible approach be preferred to a less
flexible approach? When might a less flexible approach be preferred?
    A less flexible approach would be preferred for smaller datasets and in cases where interpretability is important.Also in cases with high error variance.

6. Describe the differences between a parametric and a non-parametric
statistical learning approach. What are the advantages of a parametric
approach to regression or classification (as opposed to a nonparametric
approach)? What are its disadvantages?

    Non-parametric approaches do not make any assumptions about the underlying function that could be used to predict the data, parametric approaches make assumptions about the underlying function. Non-parametric approaches are prone to overfitting and require a large n.  

7. The table below provides a training data set containing six observations,
three predictors, and one qualitative response variable.
Obs.X1 X2 X3 Y
1   0 3 0 Red
2   2 0 0 Red
3   0 1 3 Red
4   0 1 2 Green
5   -1 0 1 Green
6   1 1 1 Red

Suppose we wish to use this data set to make a prediction for Y when
X1 = X2 = X3 = 0 using K-nearest neighbors.

(a) Compute the Euclidean distance between each observation and
the test point, X1 = X2 = X3 = 0.

    1) 3
    2) 2
    3) sqrt(10)
    4) sqrt(5)
    5) sqrt(2)
    6) sqrt(3)

(b) What is our prediction with K = 1? Why?
    Green because observation #5 is the closest neighbor

(c) What is our prediction with K = 3? Why?
    Red. Although observation 5 is Green, the two next closest, 2 and 6, are both green.

(d) If the Bayes decision boundary in this problem is highly nonlinear,
then would we expect the best value for K to be large or
small? Why?

    Small because a small k allows for a more nonlinear fit.

```{r}
summary(cars)
```

You can also embed plots, for example:

```{r, echo=FALSE}
plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
